##Create matrix with only response variable
y <- data.all$y # define response variable
# Create training matrix
x.train <- x[-test,] # define training predictor matrix
x.test <- x[test,] # define test predictor matrix
##Create individual y matrix
y.train <- y[-test] # define training response variable
y.test <- y[test] # define test response variable
#Store number of obversations for each training and test set
n.train <- dim(data.train)[1] # training sample size = 332
n.test <- dim(data.test)[1] # test sample size = 110
model.lm <- lm(y~.,data = data.train)
lm.summary <- summary(model.lm)
lm.summary
lm.pred <- predict(model.lm,data.test)
mean((lm.pred -data.test$y)^2)
sd((lm.pred -data.test$y)^2)/sqrt(length((lm.pred -data.test$y)^2))
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form ,newdata )
coefi =coef(object ,id=id)
xvars =names(coefi )
mat[,xvars ]%*% coefi
}
regfit.full=regsubsets(y~.,data.train,nvmax =10)
reg.summary <- summary(regfit.full)
which.min(reg.summary$bic)
#Explore graph
plot(reg.summary$bic ,xlab=" Number of Variables ",ylab="BIC",
type="l")
points(5, reg.summary$bic[5], col =" red",cex =2, pch =20)
plot(reg.summary$bic ,xlab=" Number of Variables ",ylab="BIC",
type="l")
points(6, reg.summary$bic[6], col =" red",cex =2, pch =20)
coef(regfit.full ,6)
regfit.pred <- predict.regsubsets(regfit.full,data.test,id=5)
mean((regfit.pred -data.test$y)^2)
sd((regfit.pred -data.test$y)^2)/sqrt(length((regfit.pred -data.test$y)^2))
k=10
seed
k=10
folds <- sample(1:k, nrow(data.train), replace = TRUE)
cv.errors <- matrix(NA ,k,10, dimnames =list(NULL , paste (1:10) ))
for(j in 1:k){
best.fit=regsubsets(y~.,data=data.train[folds !=j,],
nvmax =10)
for(i in 1:10) {
pred=predict(best.fit ,data.train[folds ==j,], id=i)
cv.errors[j,i]=mean( (data.train$y[folds ==j]-pred)^2)
}
}
mean.cv.errors =apply(cv.errors ,2, mean)
min(mean.cv.errors)
set.seed(1306)
folds <- sample(1:k, nrow(data.train), replace = TRUE)
cv.errors <- matrix(NA ,k,10, dimnames =list(NULL , paste (1:10) ))
for(j in 1:k){
best.fit=regsubsets(y~.,data=data.train[folds !=j,],
nvmax =10)
for(i in 1:10) {
pred=predict(best.fit ,data.train[folds ==j,], id=i)
cv.errors[j,i]=mean( (data.train$y[folds ==j]-pred)^2)
}
}
mean.cv.errors =apply(cv.errors ,2, mean)
min(mean.cv.errors)
k=10
set.seed(1306)
folds <- sample(1:k, nrow(data.train), replace = TRUE)
cv.errors <- matrix(NA ,k,10, dimnames =list(NULL , paste (1:10) ))
for(j in 1:k){
best.fit=regsubsets(y~.,data=data.train[folds !=j,],
nvmax =10)
for(i in 1:10) {
pred=predict(best.fit ,data.train[folds ==j,], id=i)
cv.errors[j,i]=mean( (data.train$y[folds ==j]-pred)^2)
}
}
mean.cv.errors =apply(cv.errors ,2, mean)
min(mean.cv.errors)
par(mfrow =c(1,1))
plot(mean.cv.errors ,type='b')
reg.best=regsubsets (y~.,data=data.train , nvmax =10)
coef(reg.best ,6)
regbest.pred <- predict.regsubsets(reg.best,data.test,id=6)
mean((regbest.pred -data.test$y)^2)
sd((regbest.pred -data.test$y)^2)/sqrt(length((regbest.pred -data.test$y)^2))
set.seed(1306)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
grid =10^seq(10,-2, length =100)
set.seed(1306)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
plot(ridge.mod, xvar="lambda", label=T)
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
plot(ridge.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
bestlam <- cv.out$lambda.1se
bestlam
#Mean Prediction Error
ridge.pred=predict(ridge.mod ,s=bestlam ,newx=x.test)
mean((ridge.pred -y.test)^2)
sd((ridge.pred -y.test)^2)/sqrt(length((ridge.pred -y.test)^2))
ridge.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
ridge.coef
grid =10^seq(10,-2, length =100)
lasso.mod =glmnet(x.train,y.train,alpha =1, lambda =grid)
plot(lasso.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =1)
bestlam <- cv.out$lambda.1se
bestlam
lasso.pred=predict(lasso.mod ,s=bestlam ,newx=x.test)
mean((lasso.pred -y.test)^2)
sd((lasso.pred -y.test)^2)/sqrt(length((lasso.pred -y.test)^2))
lasso.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
lasso.coef
library(lars)
library(GGally)
library(ggplot2)
library (gridExtra)
library(corrgram)
library(corrplot)
library(leaps)
library(glmnet)
data(diabetes)
data.all <- data.frame(cbind(diabetes$x, y = diabetes$y))
dim(data.all)
names(data.all)
summary(data.all)
str(data.all)
n <- dim(data.all)[1] # sample size = 442
set.seed(1306) # set random number generator seed to enable
test <- sample(n, round(n/4)) # randomly sample 25% test
data.train <- data.all[-test,]
data.test <- data.all[test,]
x <- model.matrix(y ~ ., data = data.all)[,-1] # define predictor matrix
##Create matrix with only response variable
y <- data.all$y # define response variable
# Create training matrix
x.train <- x[-test,] # define training predictor matrix
x.test <- x[test,] # define test predictor matrix
##Create individual y matrix
y.train <- y[-test] # define training response variable
y.test <- y[test] # define test response variable
#Store number of obversations for each training and test set
n.train <- dim(data.train)[1] # training sample size = 332
n.test <- dim(data.test)[1] # test sample size = 110
model.lm <- lm(y~.,data = data.train)
lm.summary <- summary(model.lm)
lm.summary
lm.pred <- predict(model.lm,data.test)
mean((lm.pred -data.test$y)^2)
sd((lm.pred -data.test$y)^2)/sqrt(length((lm.pred -data.test$y)^2))
predict.regsubsets =function (object ,newdata ,id ,...){
form=as.formula(object$call [[2]])
mat=model.matrix(form ,newdata )
coefi =coef(object ,id=id)
xvars =names(coefi )
mat[,xvars ]%*% coefi
}
regfit.full=regsubsets(y~.,data.train,nvmax =10)
reg.summary <- summary(regfit.full)
which.min(reg.summary$bic)
#Explore graph
plot(reg.summary$bic ,xlab=" Number of Variables ",ylab="BIC",
type="l")
points(6, reg.summary$bic[6], col =" red",cex =2, pch =20)
#Get variables and coefficients
coef(regfit.full ,6)
regfit.pred <- predict.regsubsets(regfit.full,data.test,id=5)
mean((regfit.pred -data.test$y)^2)
sd((regfit.pred -data.test$y)^2)/sqrt(length((regfit.pred -data.test$y)^2))
k=10
set.seed(1306)
folds <- sample(1:k, nrow(data.train), replace = TRUE)
cv.errors <- matrix(NA ,k,10, dimnames =list(NULL , paste (1:10) ))
for(j in 1:k){
best.fit=regsubsets(y~.,data=data.train[folds !=j,],
nvmax =10)
for(i in 1:10) {
pred=predict(best.fit ,data.train[folds ==j,], id=i)
cv.errors[j,i]=mean( (data.train$y[folds ==j]-pred)^2)
}
}
mean.cv.errors =apply(cv.errors ,2, mean)
min(mean.cv.errors)
# [1] 2978.907
par(mfrow =c(1,1))
plot(mean.cv.errors ,type='b')
reg.best=regsubsets (y~.,data=data.train , nvmax =10)
coef(reg.best ,6)
#Coefficients:
regbest.pred <- predict.regsubsets(reg.best,data.test,id=6)
mean((regbest.pred -data.test$y)^2)
sd((regbest.pred -data.test$y)^2)/sqrt(length((regbest.pred -data.test$y)^2))
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
bestlam <- cv.out$lambda.1se
bestlam
ridge.pred=predict(ridge.mod ,s=bestlam ,newx=x.test)
mean((ridge.pred -y.test)^2)
sd((ridge.pred -y.test)^2)/sqrt(length((ridge.pred -y.test)^2))
ridge.coef
ridge.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
ridge.coef
grid =10^seq(10,-2, length =100)
lasso.mod =glmnet(x.train,y.train,alpha =1, lambda =grid)
plot(lasso.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =1)
bestlam <- cv.out$lambda.1se
bestlam
# [1] 4.791278
#Mean Prediction Error:
lasso.pred=predict(lasso.mod ,s=bestlam ,newx=x.test)
mean((lasso.pred -y.test)^2)
# [1] 2920.08
#SE of MSE
sd((lasso.pred -y.test)^2)/sqrt(length((lasso.pred -y.test)^2))
lasso.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
lasso.coef
save.image("~/Documents/Work_Computer/Grad_School/PREDICT_422/Module4/IndividualProject1/Project1.RData")
p1<-ggplot(aes(x=age),
data = data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Age')
p2<-ggplot(aes(x=sex),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Gender')
p3<-ggplot(aes(x=bmi),
data =  data.all ) +
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('BMI')
p4<-ggplot(aes(x=map),data=data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Avg Blood Pressure')
p5<-ggplot(aes(x=tc),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('tc')
p6<-ggplot(aes(x=ldl),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('LDL')
p7<-ggplot(aes(x=hdl),
data =  data.all ) +
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('HDL')
p8<-ggplot(aes(x=tch),data=data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('TCH')
p9<-ggplot(aes(x=ltg),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('LTG')
p10<-ggplot(aes(x=glu),
data =  data.all)+
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Glucose')
p11<-ggplot(aes(x=y),
data =  data.all ) +
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Quantitative Progression of Disease')
grid.arrange(p1,p2,p3,p3,p4,p5,p6,p7,p8,p9,p10,p11,ncol=3)
#There is no missing data, all variables are numeric, there are 442 data points and 11 variables
p11<-ggplot(aes(x=y),           data =  data.all ) +  geom_histogram(color =I('black'),fill = I('#099009'))+  ggtitle('Disease Progression')
p11<-ggplot(aes(x=y),
data =  data.all ) +
geom_histogram(color =I('black'),fill = I('#099009'))+
ggtitle('Disease Progression')
grid.arrange(p1,p2,p3,p3,p4,p5,p6,p7,p8,p9,p10,p11,ncol=3)
dim(data.all)
grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10,p11,ncol=3)
coef(regfit.full ,6)
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
plot(ridge.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
bestlam <- cv.out$lambda.1se
bestlam
plot(cv.out)
grid =10^seq(10,-2, length =100)
lasso.mod =glmnet(x.train,y.train,alpha =1, lambda =grid)
plot(lasso.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =1)
bestlam <- cv.out$lambda.1se
bestlam
plot(cv.out)
log(bestlam)
plot(lasso.mod, xvar="lambda", label=T)
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
plot(ridge.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
bestlam <- cv.out$lambda.1se
bestlam
log(bestlam)
grid =10^seq(10,-2, length =100)
ridge.mod =glmnet(x.train,y.train,alpha =0, lambda =grid)
plot(ridge.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =0)
plot(cv.out)
ridge.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:10 ,]
ridge.coef
ridge.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:11,]
ridge.coef
lasso.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:11 ,]
lasso.coef
grid =10^seq(10,-2, length =100)
lasso.mod =glmnet(x.train,y.train,alpha =1, lambda =grid)
plot(lasso.mod, xvar="lambda", label=T)
set.seed(1306)
cv.out <- cv.glmnet(x.train,y.train,alpha =1)
bestlam <- cv.out$lambda.1se
bestlam
# [1] 4.791278
#Mean Prediction Error:
lasso.pred=predict(lasso.mod ,s=bestlam ,newx=x.test)
mean((lasso.pred -y.test)^2)
# [1] 2920.08
#SE of MSE
sd((lasso.pred -y.test)^2)/sqrt(length((lasso.pred -y.test)^2))
# [1] 346.228
##Coefficients:
lasso.coef=predict(cv.out ,type ="coefficients",s=bestlam )[1:11 ,]
lasso.coef
regfit.pred <- predict.regsubsets(regfit.full,data.test,id=6)
mean((regfit.pred -data.test$y)^2)
sd((regfit.pred -data.test$y)^2)/sqrt(length((regfit.pred -data.test$y)^2))
save.image("~/Documents/Work_Computer/Grad_School/PREDICT_422/Module4/IndividualProject1/Project1.RData")
oib_isp <- summaryBy(inbox_rate ~ COMMON_NAME,data=oib,FUN= mean,na.rm=TRUE)
library(doBy)
oib_isp <- summaryBy(inbox_rate ~ COMMON_NAME,data=oib,FUN= mean,na.rm=TRUE)
########################################################################
#Name: Group 2 - Bruckner, Funk, Sheets, Ulrich                        #
#Due Date: December 4th, 2016                                          #
#Course: PREDICT 422                                                   #
#Section: 56                                                           #
#Assignment: Group Project                                             #
#Goal: We would like to develop a classification model using data from #
#      the most recent campaign that can effectively captures likely   #
#      donors so that the expected net profit is maximized. We would   #
#      also like to build a prediction model to predict expected gift  #
#      amounts from donors – the data for this will consist of the     #
#      records for donors only.                                        #
########################################################################
#Variables
# ID number [Do NOT use this as a predictor variable in any models]
# REG1, REG2, REG3, REG4: Region (There are five geographic regions; A 1 indicates the potential donor belongs to this region.)
# HOME: (1 = homeowner, 0 = not a homeowner)
# CHLD: Number of children
# HINC: Household income (7 categories)
# GENF: Gender (0 = Male, 1 = Female)
# WRAT: Wealth Rating (Wealth rating uses median family income and population statistics from each area to index relative wealth within each state. The segments are denoted 0-9, with 9 being the highest wealth group and 0 being the lowest.)
# AVHV: Average Home Value in potential donor's neighborhood in $ thousands
# INCM: Median Family Income in potential donor's neighborhood in $ thousands
# INCA: Average Family Income in potential donor's neighborhood in $ thousands
# PLOW: Percent categorized as “low income” in potential donor's neighborhood
# NPRO: Lifetime number of promotions received to date
# TGIF: Dollar amount of lifetime gifts to date
# LGIF: Dollar amount of largest gift to date
# RGIF: Dollar amount of most recent gift
# TDON: Number of months since last donation
# TLAG: Number of months between first and second gift
# AGIF: Average dollar amount of gifts to date
# DONR: Classification Response Variable (1 = Donor, 0 = Non-donor)
# DAMT: Prediction Response Variable (Donation Amount in $).
setwd("/Users/asheets/Documents/Work_Computer/Grad_School/PREDICT_422/PREDICT422_GroupProject")
set.seed(1)
#Install Packages
library(doBy)
library(psych)
library(lars)
library(GGally)
library(ggplot2)
library (gridExtra)
library(corrgram)
library(corrplot)
library(leaps)
library(glmnet)
library(MASS)
library(gbm)
library(tree)
library(rpart)
library(rpart.plot)
library(gam)
library(class)
library(e1071)
library(randomForest)
# Load the diabetes data
data <- read.csv(file="charity.csv",stringsAsFactors=FALSE,header=TRUE,quote="",comment.char="")
#Explore the data -- how big is it, what types of variables included, distributions and missing values.
dim(data)
names(data)
summary(data)
str(data)
charity.t <- data
#These variables are all skewed right
charity.t$avhv <- log(charity.t$avhv)
charity.t$inca <- log(charity.t$inca)
charity.t$incm <- log(charity.t$incm)
charity.t$agif <- log(charity.t$agif)
charity.t$rgif <- log(charity.t$rgif)
charity.t$lgif <- log(charity.t$lgif)
data.train <- charity.t[charity.t$part=="train",]
x.train <- data.train[,2:21]
c.train <- data.train[,22] # donr
n.train.c <- length(c.train) # 3984
y.train <- data.train[c.train==1,23] # damt for observations with donr=1
n.train.y <- length(y.train) # 1995
data.valid <- charity.t[charity.t$part=="valid",]
x.valid <- data.valid[,2:21]
c.valid <- data.valid[,22] # donr
n.valid.c <- length(c.valid) # 2018
y.valid <- data.valid[c.valid==1,23] # damt for observations with donr=1
n.valid.y <- length(y.valid) # 999
data.test <- charity.t[charity.t$part=="test",]
n.test <- dim(data.test)[1] # 2007
x.test <- data.test[,2:21]
x.train.mean <- apply(x.train, 2, mean)
x.train.sd <- apply(x.train, 2, sd)
x.train.std <- t((t(x.train)-x.train.mean)/x.train.sd) # standardize to have zero mean and unit sd
apply(x.train.std, 2, mean) # check zero mean
apply(x.train.std, 2, sd) # check unit sd
data.train.std.c <- data.frame(x.train.std, donr=c.train) # to classify donr
data.train.std.y <- data.frame(x.train.std[c.train==1,], damt=y.train) # to predict damt when donr=1
x.valid.std <- t((t(x.valid)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.valid.std.c <- data.frame(x.valid.std, donr=c.valid) # to classify donr
data.valid.std.y <- data.frame(x.valid.std[c.valid==1,], damt=y.valid) # to predict damt when donr=1
x.test.std <- t((t(x.test)-x.train.mean)/x.train.sd) # standardize using training mean and sd
data.test.std <- data.frame(x.test.std)
k=10
set.seed(1)
folds <- sample(1:k, nrow(data.train), replace = TRUE)
cv.errors <- matrix(NA,k,10, dimnames=list(NULL, paste(1:10)))
for(j in 1:k){
model.bestsub1 <- regsubsets(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data = data.train.std.y[folds !=j,],
nvmax =20)
for(i in 1:10) {
pred <- predict(model.bestsub1, data.train.std.y[folds==j,], id=i)
cv.errors[j,i] <- mean((data.train.std.y$damt[folds==j]-pred)^2)
}
}
k=10
set.seed(1)
folds <- sample(1:k, nrow(data.train.std.y), replace = TRUE)
cv.errors <- matrix(NA,k,10, dimnames=list(NULL, paste(1:10)))
for(j in 1:k){
model.bestsub1 <- regsubsets(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data = data.train.std.y[folds !=j,],
nvmax =20)
for(i in 1:10) {
pred <- predict(model.bestsub1, data.train.std.y[folds==j,], id=i)
cv.errors[j,i] <- mean((data.train.std.y$damt[folds==j]-pred)^2)
}
}
k=10
set.seed(1)
folds <- sample(1:k, nrow(data.train.std.y), replace = TRUE)
cv.errors <- matrix(NA,k,10, dimnames=list(NULL, paste(1:10)))
for(j in 1:k){
model.bestsub1 <- regsubsets(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data = data.train.std.y[folds !=j,],
nvmax =20)
print j
for(i in 1:10) {
pred <- predict(model.bestsub1, data.train.std.y[folds==j,], id=i)
cv.errors[j,i] <- mean((data.train.std.y$damt[folds==j]-pred)^2)
}
}
k=10
set.seed(1)
folds <- sample(1:k, nrow(data.train.std.y), replace = TRUE)
cv.errors <- matrix(NA,k,10, dimnames=list(NULL, paste(1:10)))
for(j in 1:k){
model.bestsub1 <- regsubsets(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data = data.train.std.y[folds !=j,],
nvmax =20)
print(j)
for(i in 1:10) {
pred <- predict(model.bestsub1, data.train.std.y[folds==j,], id=i)
cv.errors[j,i] <- mean((data.train.std.y$damt[folds==j]-pred)^2)
}
}
j = 1
regsubsets(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data = data.train.std.y[folds !=j,],
nvmax =20)
model.bestsub1 <- regsubsets(damt ~ reg1 + reg2 + reg3 + reg4 + home + chld + hinc + genf + wrat +
avhv + incm + inca + plow + npro + tgif + lgif + rgif + tdon + tlag + agif,
data = data.train.std.y[folds !=j,],
nvmax =20)
summary(model.bestsub1)
i = 1
predict(model.bestsub1, data.train.std.y[folds==j,], id=i)
class(model.bestsub1)
?regsubsets
